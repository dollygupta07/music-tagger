{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Robustness.ipynb","provenance":[{"file_id":"19zhdAlVB4Xfz3NkarbyuxPcHkVSWiXFc","timestamp":1637688148309}],"authorship_tag":"ABX9TyP7qG0lxypMDbBTjjp27cIG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"k1pEPKycUzvm"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B0LR9vG50Dsw"},"source":["# coding: utf-8\n","'''\n","Deformation codes are borrowed from MUDA\n","McFee et al., A software framework for musical data augmentation, 2015\n","https://github.com/bmcfee/muda\n","'''\n","import os\n","import time\n","import subprocess\n","import tempfile\n","import numpy as np\n","import pandas as pd\n","import datetime\n","import tqdm\n","import csv\n","import fire\n","import argparse\n","import pickle\n","from sklearn import metrics\n","import pandas as pd\n","import librosa\n","import soundfile as psf\n","\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from solver import skip_files\n","from sklearn.preprocessing import LabelBinarizer\n","\n","import model as Model\n","\n","\n","TAGS = ['genre---downtempo', 'genre---ambient', 'genre---rock', 'instrument---synthesizer', 'genre---atmospheric', 'genre---indie', 'instrument---electricpiano', 'genre---newage', 'instrument---strings', 'instrument---drums', 'instrument---drummachine', 'genre---techno', 'instrument---guitar', 'genre---alternative', 'genre---easylistening', 'genre---instrumentalpop', 'genre---chillout', 'genre---metal', 'mood/theme---happy', 'genre---lounge', 'genre---reggae', 'genre---popfolk', 'genre---orchestral', 'instrument---acousticguitar', 'genre---poprock', 'instrument---piano', 'genre---trance', 'genre---dance', 'instrument---electricguitar', 'genre---soundtrack', 'genre---house', 'genre---hiphop', 'genre---classical', 'mood/theme---energetic', 'genre---electronic', 'genre---world', 'genre---experimental', 'instrument---violin', 'genre---folk', 'mood/theme---emotional', 'instrument---voice', 'instrument---keyboard', 'genre---pop', 'instrument---bass', 'instrument---computer', 'mood/theme---film', 'genre---triphop', 'genre---jazz', 'genre---funk', 'mood/theme---relaxing']\n","\n","def read_file(tsv_file):\n","    tracks = {}\n","    with open(tsv_file) as fp:\n","        reader = csv.reader(fp, delimiter='\\t')\n","        next(reader, None)  # skip header\n","        for row in reader:\n","            track_id = row[0]\n","            tracks[track_id] = {\n","                'path': row[3].replace('.mp3', '.npy'),\n","                'tags': row[5:],\n","            }\n","    return tracks\n","\n","\n","class Predict(object):\n","    def __init__(self, config):\n","        self.model_type = config.model_type\n","        self.model_load_path = config.model_load_path\n","        self.dataset = config.dataset\n","        self.data_path = config.data_path\n","        self.batch_size = config.batch_size\n","        self.is_cuda = torch.cuda.is_available()\n","        self.build_model()\n","        self.get_dataset()\n","        self.mod = config.mod\n","        self.rate = config.rate\n","        self.PRESETS = {\n","                        \"radio\":            [\"0.01,1\", \"-90,-90,-70,-70,-60,-20,0,0\", \"-5\"],\n","                        \"film standard\":    [\"0.1,0.3\", \"-90,-90,-70,-64,-43,-37,-31,-31,-21,-21,0,-20\", \"0\", \"0\", \"0.1\"],\n","                        \"film light\":       [\"0.1,0.3\", \"-90,-90,-70,-64,-53,-47,-41,-41,-21,-21,0,-20\", \"0\", \"0\", \"0.1\"],\n","                        \"music standard\":   [\"0.1,0.3\", \"-90,-90,-70,-58,-55,-43,-31,-31,-21,-21,0,-20\", \"0\", \"0\", \"0.1\"],\n","                        \"music light\":      [\"0.1,0.3\", \"-90,-90,-70,-58,-65,-53,-41,-41,-21,-21,0,-11\", \"0\", \"0\", \"0.1\"],\n","                        \"speech\":           [\"0.1,0.3\", \"-90,-90,-70,-55,-50,-35,-31,-31,-21,-21,0,-20\", \"0\", \"0\", \"0.1\"]\n","                        }\n","        self.preset_dict = {1: \"radio\",\n","                            2: \"film standard\",\n","                            3: \"film light\",\n","                            4: \"music standard\",\n","                            5: \"music light\",\n","                            6: \"speech\"}\n","    def get_model(self):\n","        if self.model_type == 'fcn':\n","            self.input_length = 29 * 16000\n","            return Model.FCN()\n","        elif self.model_type == 'musicnn':\n","            self.input_length = 3 * 16000\n","            return Model.Musicnn(dataset=self.dataset)\n","        elif self.model_type == 'crnn':\n","            self.input_length = 29 * 16000\n","            return Model.CRNN()\n","        elif self.model_type == 'sample':\n","            self.input_length = 59049\n","            return Model.SampleCNN()\n","        elif self.model_type == 'se':\n","            self.input_length = 59049\n","            return Model.SampleCNNSE()\n","        elif self.model_type == 'short':\n","            self.input_length = 59049\n","            return Model.ShortChunkCNN()\n","        elif self.model_type == 'short_res':\n","            self.input_length = 59049\n","            return Model.ShortChunkCNN_Res()\n","        elif self.model_type == 'attention':\n","            self.input_length = 15 * 16000\n","            return Model.CNNSA()\n","        elif self.model_type == 'hcnn':\n","            self.input_length = 5 * 16000\n","            return Model.HarmonicCNN()\n","        else:\n","            print('model_type has to be one of [fcn, musicnn, crnn, sample, se, short, short_res, attention]')\n","\n","    def build_model(self):\n","        self.model = self.get_model()\n","\n","        # cuda\n","        if self.is_cuda:\n","            self.model.cuda()\n","\n","        # load model\n","        self.load(self.model_load_path)\n","\n","    def get_dataset(self):\n","        if self.dataset == 'mtat':\n","            self.test_list = np.load('./../split/mtat/test.npy')\n","            self.binary = np.load('./../split/mtat/binary.npy')\n","        if self.dataset == 'msd':\n","            test_file = os.path.join('./../split/msd','filtered_list_test.cP')\n","            test_list = pickle.load(open(test_file,'rb'), encoding='bytes')\n","            self.test_list = [value for value in test_list if value.decode() not in skip_files]\n","            id2tag_file = os.path.join('./../split/msd', 'msd_id_to_tag_vector.cP')\n","            self.id2tag = pickle.load(open(id2tag_file,'rb'), encoding='bytes')\n","        if self.dataset == 'jamendo':\n","            test_file = os.path.join('./../split/mtg-jamendo', 'autotagging_top50tags-test.tsv')\n","            self.file_dict= read_file(test_file)\n","            self.test_list= list(self.file_dict.keys())\n","            self.mlb = LabelBinarizer().fit(TAGS)\n","\n","    def load(self, filename):\n","        S = torch.load(filename)\n","        self.model.load_state_dict(S)\n","\n","    def to_var(self, x):\n","        if torch.cuda.is_available():\n","            x = x.cuda()\n","        return Variable(x)\n","\n","    def get_tensor(self, fn):\n","        # load audio\n","        if self.dataset == 'mtat':\n","            npy_path = os.path.join(self.data_path, 'mtat', 'npy', fn.split('/')[1][:-3]) + 'npy'\n","        elif self.dataset == 'msd':\n","            msid = fn.decode()\n","            filename = '{}/{}/{}/{}.npy'.format(msid[2], msid[3], msid[4], msid)\n","            npy_path = os.path.join(self.data_path, filename)\n","        elif self.dataset == 'jamendo':\n","            filename = self.file_dict[fn]['path']\n","            npy_path = os.path.join(self.data_path, filename)\n","        raw = np.load(npy_path, mmap_mode='r')\n","        raw = self.modify(raw, self.rate, self.mod)\n","\n","        # split chunk\n","        length = len(raw)\n","        hop = (length - self.input_length) // self.batch_size\n","        x = torch.zeros(self.batch_size, self.input_length)\n","        for i in range(self.batch_size):\n","            x[i] = torch.Tensor(raw[i*hop:i*hop+self.input_length]).unsqueeze(0)\n","        return x\n","\n","    def modify(self, x, mod_rate, mod_type):\n","        if mod_type == 'time_stretch':\n","            return self.time_stretch(x, mod_rate)\n","        elif mod_type == 'pitch_shift':\n","            return self.pitch_shift(x, mod_rate)\n","        elif mod_type == 'dynamic_range':\n","            return self.dynamic_range_compression(x, mod_rate)\n","        elif mod_type == 'white_noise':\n","            return self.white_noise(x, mod_rate)\n","        else:\n","            print('choose from [time_stretch, pitch_shift, dynamic_range, white_noise]')\n","\n","    def time_stretch(self, x, rate):\n","        '''\n","        [2 ** (-.5), 2 ** (.5)]\n","        '''\n","        return librosa.effects.time_stretch(x, rate)\n","\n","    def pitch_shift(self, x, rate):\n","        '''\n","        [-1, 1]\n","        '''\n","        return librosa.effects.pitch_shift(x, 16000, rate)\n","\n","    def dynamic_range_compression(self, x, rate):\n","        '''\n","        [4, 6]\n","        Music standard & Speech\n","        '''\n","        return self.sox(x, 16000, \"compand\", *self.PRESETS[self.preset_dict[rate]])\n","\n","    @staticmethod\n","    def sox(x, fs, *args):\n","        assert fs > 0\n","\n","        fdesc, infile = tempfile.mkstemp(suffix=\".wav\")\n","        os.close(fdesc)\n","        fdesc, outfile = tempfile.mkstemp(suffix=\".wav\")\n","        os.close(fdesc)\n","\n","        psf.write(infile, x, fs)\n","\n","        try:\n","            arguments = [\"sox\", infile, outfile, \"-q\"]\n","            arguments.extend(args)\n","\n","            subprocess.check_call(arguments)\n","\n","            x_out, fs = psf.read(outfile)\n","            x_out = x_out.T\n","            if x.ndim == 1:\n","                x_out = librosa.to_mono(x_out)\n","\n","        finally:\n","            os.unlink(infile)\n","            os.unlink(outfile)\n","\n","        return x_out\n","\n","    def white_noise(self, x, rate):\n","        '''\n","        [0.1, 0.4]\n","        '''\n","        n_frames = len(x)\n","        noise_white = np.random.RandomState().randn(n_frames)\n","        noise_fft = np.fft.rfft(noise_white)\n","        values = np.linspace(1, n_frames * 0.5 + 1, n_frames // 2 + 1)\n","        colored_filter = np.linspace(1, n_frames / 2 + 1, n_frames // 2 + 1) ** 0\n","        noise_filtered = noise_fft * colored_filter\n","        noise = librosa.util.normalize(np.fft.irfft(noise_filtered)) * (x.max())\n","        if len(noise) < len(x):\n","            x = x[:len(noise)]\n","        return (1 - rate) * x + (noise * rate)\n","\n","    def get_auc(self, est_array, gt_array):\n","        roc_aucs  = metrics.roc_auc_score(gt_array, est_array, average='macro')\n","        pr_aucs = metrics.average_precision_score(gt_array, est_array, average='macro')\n","        return roc_aucs, pr_aucs\n","\n","    def test(self):\n","        roc_auc, pr_auc, loss = self.get_test_score()\n","        print('loss: %.4f' % loss)\n","        print('roc_auc: %.4f' % roc_auc)\n","        print('pr_auc: %.4f' % pr_auc)\n","\n","    def get_test_score(self):\n","        self.model = self.model.eval()\n","        est_array = []\n","        gt_array = []\n","        losses = []\n","        reconst_loss = nn.BCELoss()\n","        for line in tqdm.tqdm(self.test_list):\n","            if self.dataset == 'mtat':\n","                ix, fn = line.split('\\t')\n","            elif self.dataset == 'msd':\n","                fn = line\n","                if fn.decode() in skip_files:\n","                    continue\n","            elif self.dataset == 'jamendo':\n","                fn = line\n","\n","            # load and split\n","            x = self.get_tensor(fn)\n","\n","            # ground truth\n","            if self.dataset == 'mtat':\n","                ground_truth = self.binary[int(ix)]\n","            elif self.dataset == 'msd':\n","                ground_truth = self.id2tag[fn].flatten()\n","            elif self.dataset == 'jamendo':\n","                ground_truth = np.sum(self.mlb.transform(self.file_dict[fn]['tags']), axis=0)\n","\n","            # forward\n","            x = self.to_var(x)\n","            y = torch.tensor([ground_truth.astype('float32') for i in range(self.batch_size)]).cuda()\n","            out = self.model(x)\n","            loss = reconst_loss(out, y)\n","            losses.append(float(loss.data))\n","            out = out.detach().cpu()\n","\n","            # estimate\n","            estimated = np.array(out).mean(axis=0)\n","            est_array.append(estimated)\n","            gt_array.append(ground_truth)\n","\n","        est_array, gt_array = np.array(est_array), np.array(gt_array)\n","        loss = np.mean(losses)\n","\n","        roc_auc, pr_auc = self.get_auc(est_array, gt_array)\n","        return roc_auc, pr_auc, loss\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n","\n","    parser.add_argument('--num_workers', type=int, default=0)\n","    parser.add_argument('--dataset', type=str, default='mtat', choices=['mtat', 'msd', 'jamendo'])\n","    parser.add_argument('--model_type', type=str, default='fcn',\n","                        choices=['fcn', 'musicnn', 'crnn', 'sample', 'se', 'short', 'short_res', 'attention', 'hcnn'])\n","    parser.add_argument('--batch_size', type=int, default=16)\n","    parser.add_argument('--model_load_path', type=str, default='.')\n","    parser.add_argument('--data_path', type=str, default='./data')\n","    parser.add_argument('--mod', type=str, default='time_stretch')\n","    parser.add_argument('--rate', type=float, default=0)\n","\n","    config = parser.parse_args()\n","\n","    p = Predict(config)\n","    p.test()\n"],"execution_count":null,"outputs":[]}]}